{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from typing import Iterable, Iterator, Mapping, Set, Tuple, Callable\n",
    "from rl.markov_process import NonTerminal\n",
    "from rl.approximate_dynamic_programming import S, A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Question 2a. (Tabular SARSA with GLIE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from rl.distribution import Categorical\n",
    "from rl.markov_decision_process import MarkovDecisionProcess\n",
    "from rl.approximate_dynamic_programming import NTStateDistribution\n",
    "from collections import defaultdict\n",
    "#Epsilon greedy for tabular case\n",
    "def epsilon_greedy_action_Tabular(\n",
    "    q: Mapping[Tuple[S, A],float],\n",
    "    nt_state: NonTerminal[S],\n",
    "    actions: Set[A],\n",
    "    epsilon: float\n",
    "): #-> void\n",
    "    greedy_action: A = max(\n",
    "        ((a, q[(nt_state, a)]) for a in actions),\n",
    "        key = itemgetter(1)\n",
    "    )[0]\n",
    "    return Categorical(\n",
    "        {a: epsilon / len(actions) +\n",
    "         (1 - epsilon if a == greedy_action else 0.) for a in actions}\n",
    "    ).sample()\n",
    "\n",
    "#implementation of tabular sarsa\n",
    "def glie_sarsa_Tabular(\n",
    "        mdp: MarkovDecisionProcess[S,A],\n",
    "        ini_dist: NTStateDistribution[S],\n",
    "        γ: float,\n",
    "        epsilon_as_func_of_episodes: Callable[[int], float],\n",
    "        max_episode_length: int,\n",
    "        alph = 0.03,\n",
    "        beta = 0.5,\n",
    "        H = 1000\n",
    ") -> Iterator[Mapping[Tuple[S,A], float]]:\n",
    "    count_to_weight_func: Callable[[int], float] = lambda n: alph/(1+((n-1)/H)**beta)\n",
    "    q: Mapping[Tuple[S,A], float] = defaultdict(lambda: 0.)\n",
    "    yield q\n",
    "\n",
    "    total_counts: int = 0\n",
    "    epi_counts: int = 0\n",
    "    while True: #loop for one episode\n",
    "        #initilization for one loop of episode\n",
    "        epi_counts += 1 #counting loops\n",
    "        epsilon: float = epsilon_as_func_of_episodes(epi_counts)\n",
    "        state: NonTerminal[S] = ini_dist.sample() #initial state\n",
    "        action: A = epsilon_greedy_action_Tabular(\n",
    "                    q = q,\n",
    "                    nt_state = state,\n",
    "                    actions = set(mdp.actions(state)),\n",
    "                    epsilon = epsilon\n",
    "        )\n",
    "        for _ in range(max_episode_length):\n",
    "            total_counts += 1\n",
    "            next_state, reward = mdp.step(state, action).sample()\n",
    "            if isinstance(next_state, NonTerminal):\n",
    "                #ε-greedy\n",
    "                next_action: A = epsilon_greedy_action_Tabular(\n",
    "                                 q = q,\n",
    "                                 nt_state = next_state,\n",
    "                                 actions = set(mdp.actions(next_state)),\n",
    "                                 epsilon = epsilon\n",
    "                )\n",
    "                #Q-update\n",
    "                weight: float = count_to_weight_func(total_counts)\n",
    "                q[(state, action)] += weight * \\\n",
    "                                      (reward + γ * q[(next_state, next_action)] - q[(state, action)])\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "                yield q\n",
    "            else:\n",
    "                #Q-update\n",
    "                weight: float = count_to_weight_func(total_counts)\n",
    "                q[(state, action)] += weight * (reward - q[(state, action)])\n",
    "                state = next_state\n",
    "\n",
    "                yield q\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 3.a (Tabular Q-learning)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def q_learning_Tabular(\n",
    "        mdp: MarkovDecisionProcess[S,A],\n",
    "        #policy_from_q is ε-greedy\n",
    "        ini_dist: NTStateDistribution[S],\n",
    "        γ: float,\n",
    "        epsilon_as_func_of_episodes: Callable[[int], float],\n",
    "        max_episode_length: float,\n",
    "        alph = 0.03,\n",
    "        beta = 0.5,\n",
    "        H = 1000\n",
    ") -> Iterator[Mapping[Tuple[S,A],float]]:\n",
    "    count_to_weight_func: Callable[[int], float] = lambda n: alph/(1+((n-1)/H)**beta)\n",
    "    q: Mapping[Tuple[S,A], float] = defaultdict(lambda: 0.)\n",
    "\n",
    "    yield q\n",
    "\n",
    "    total_counts: int = 0\n",
    "    epi_counts: int = 0\n",
    "    while True: #loop for one episode\n",
    "        epi_counts += 1\n",
    "        epsilon: float = epsilon_as_func_of_episodes(epi_counts)\n",
    "        state: NonTerminal[S] = ini_dist.sample()\n",
    "        for _ in range(max_episode_length):\n",
    "            total_counts += 1\n",
    "            if isinstance(state, NonTerminal):\n",
    "                action = epsilon_greedy_action_Tabular(\n",
    "                q = q,\n",
    "                nt_state = state,\n",
    "                actions = set(mdp.actions(state)),\n",
    "                epsilon = epsilon\n",
    "                )\n",
    "\n",
    "                next_state, reward = mdp.step(state, action).sample()\n",
    "                greedy_reward = max(\n",
    "                    ((a, q[(next_state, a)]) for a in mdp.actions(next_state)),\n",
    "                    key = itemgetter(1)\n",
    "                )[1]\n",
    "                #Q-update\n",
    "                weight = count_to_weight_func(total_counts)\n",
    "                q[(state, action)] += weight * (reward + γ * greedy_reward - q[(state, action)])\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                yield q\n",
    "\n",
    "            else:\n",
    "                #Q-update\n",
    "                weight: float = count_to_weight_func(total_counts)\n",
    "                q[(state, action)] += weight * (reward - q[(state, action)])\n",
    "                state = next_state\n",
    "\n",
    "                yield q\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test against DP on SimpleInventoryMDPCap in rl/chapter3/simple inventory mdp cap.py."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "#construct test process\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap\n",
    "from pprint import pprint\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[S, int] =\\\n",
    "    SimpleInventoryMDPCap(\n",
    "        capacity=user_capacity,\n",
    "        poisson_lambda=user_poisson_lambda,\n",
    "        holding_cost=user_holding_cost,\n",
    "        stockout_cost=user_stockout_cost\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DP from BOOK\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "from rl.distribution import Choose\n",
    "import itertools\n",
    "import rl.iterate as iterate\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "\n",
    "opt_vf_pi, opt_policy_pi = policy_iteration_result(\n",
    "        si_mdp,\n",
    "        gamma=user_gamma\n",
    "    )\n",
    "\n",
    "print(\"DP from the book:\\n\")\n",
    "pprint(opt_vf_pi)\n",
    "print(opt_policy_pi)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-made GLIE SARSA in Tabular version\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.02253108881253,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.131535792838644,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.746767672058823,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.75600029052218,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.197936416749073,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.172902700207782}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#self-made Tabular SARSA\n",
    "max_episode_length: int = 100\n",
    "epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -0.5\n",
    "qvfs_Tabular: Iterator[Mapping[Tuple[S, int], float]] = glie_sarsa_Tabular(\n",
    "              mdp = si_mdp,\n",
    "              ini_dist = Choose(si_mdp.non_terminal_states),\n",
    "              γ = user_gamma,\n",
    "              epsilon_as_func_of_episodes = epsilon_as_func_of_episodes,\n",
    "              max_episode_length = max_episode_length\n",
    ")\n",
    "\n",
    "num_episodes: int = 10000\n",
    "num_updates:int = num_episodes * max_episode_length\n",
    "final_qvfs_Tabular: Mapping[Tuple[S,int], float] =\\\n",
    "     iterate.last(itertools.islice(qvfs_Tabular, num_updates))\n",
    "opt_vf_sarsa_Tabular: Mapping[S,float] = {\n",
    "        s: max(final_qvfs_Tabular[(s, a)] for a in si_mdp.actions(s))\n",
    "        for s in si_mdp.non_terminal_states\n",
    "    }\n",
    "opt_policy_sarsa_Tabular: Mapping[S, int] = \\\n",
    "    FiniteDeterministicPolicy({\n",
    "        s.state: max(\n",
    "        ((a, final_qvfs_Tabular[(s, a)]) for a in si_mdp.actions(s)),\n",
    "        key = itemgetter(1))[0]\n",
    "        for s in si_mdp.non_terminal_states\n",
    "    })\n",
    "\n",
    "print(\"Self-made GLIE SARSA in Tabular version\")\n",
    "pprint(opt_vf_sarsa_Tabular)\n",
    "pprint(opt_policy_sarsa_Tabular)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-made Q-learning in Tabular version\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.86137720551516,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.983249350738138,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.639134957723886,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.65995403367464,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.016427795889733,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.96233099982073}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#self-made Tabular Q-learning\n",
    "max_episode_length: int = 100\n",
    "epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -0.5\n",
    "qvfs_qlearning_Tabular: Iterator[Mapping[Tuple[S, int], float]] = q_learning_Tabular(\n",
    "              mdp = si_mdp,\n",
    "              ini_dist = Choose(si_mdp.non_terminal_states),\n",
    "              γ = user_gamma,\n",
    "              epsilon_as_func_of_episodes = epsilon_as_func_of_episodes,\n",
    "              max_episode_length = max_episode_length\n",
    ")\n",
    "\n",
    "num_episodes: int = 10000\n",
    "num_updates:int = num_episodes * max_episode_length\n",
    "final_qvfs_qlearning_Tabular: Mapping[Tuple[S,int], float] =\\\n",
    "     iterate.last(itertools.islice(qvfs_qlearning_Tabular, num_updates))\n",
    "opt_vf_qlearning_Tabular: Mapping[S,float] = {\n",
    "        s: max(final_qvfs_qlearning_Tabular[(s, a)] for a in si_mdp.actions(s))\n",
    "        for s in si_mdp.non_terminal_states\n",
    "    }\n",
    "opt_policy_qlearning_Tabular: Mapping[S, int] = \\\n",
    "    FiniteDeterministicPolicy({\n",
    "        s.state: max(\n",
    "        ((a, final_qvfs_qlearning_Tabular[(s, a)]) for a in si_mdp.actions(s)),\n",
    "        key = itemgetter(1))[0]\n",
    "        for s in si_mdp.non_terminal_states\n",
    "    })\n",
    "\n",
    "print(\"Self-made Q-learning in Tabular version\")\n",
    "pprint(opt_vf_qlearning_Tabular)\n",
    "pprint(opt_policy_qlearning_Tabular)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}